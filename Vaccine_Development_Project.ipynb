{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzptQhdh0pUr"
      },
      "source": [
        "# Vaccine Development with Dynamic Programming\n",
        "\n",
        "As the CEO of a biotech company which is considering the development of a new vaccine. Starting at phase 0 (state 0), the drug develpment can stay in the same state or advance to \"phase 1  with promising results\" (state 1) or advance to \"phase 1 with disappointing results\" (state 2), or fail completely (state 4). At phase 1, the drug can stay in the same state, fail or become a success (state 3), in which case you will sell its patent to a big pharma company for \\$10 million.\n",
        "These state transitions happen from month to month, and at each state, you have the option to make an additional investment of \\$100,000, which increases the chances of success.\n",
        "\n",
        "After careful study, your analysts develop the program below to simulate different scenarios using statistical data from similar projects.\n",
        "\n",
        "Use a discount factor of 0.996.\n",
        "\n",
        "-  Write a policy iteration or value iteration code to compute the value of this project. Please print the full V (value function) vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnAvrShs6ecs"
      },
      "source": [
        "import numpy as np\n",
        "class MDP():\n",
        "  def __init__(self):\n",
        "    self.A = [0, 1]\n",
        "    self.S = [0, 1, 2, 3, 4]\n",
        "\n",
        "    P0 = np.array([[0.5, .15, .15, 0, .20],\n",
        "                   [0, .5, .0, .25, .25],\n",
        "                   [0, 0, .15, .05, .8],\n",
        "                   [0, 0, 0, 0, 1],\n",
        "                   [0, 0, 0, 0, 1]])\n",
        "\n",
        "    R0 = np.array([0, 0, 0, 10, 0])\n",
        "\n",
        "    P1 = np.array([[0.5, .25, .15, 0, .10],\n",
        "                   [0, .5, .0, .35, .15],\n",
        "                   [0, 0, .20, .05, .75],\n",
        "                   [0, 0, 0, 0, 1],\n",
        "                   [0, 0, 0, 0, 1]])\n",
        "\n",
        "    R1 = np.array([-0.1, -0.1, -0.1, 10, 0])\n",
        "\n",
        "    self.P = [P0, P1]\n",
        "    self.R = [R0, R1]\n",
        "\n",
        "  def step(self, s, a):\n",
        "    s_prime = np.random.choice(len(self.S), p=self.P[a][s])\n",
        "    R = self.R[a][s]\n",
        "    if s_prime == 4:\n",
        "      done = True\n",
        "    else:\n",
        "      done = False\n",
        "    return s_prime, R, done\n",
        "\n",
        "  def simulate(self, s, a, π):\n",
        "    done = False\n",
        "    t = 0\n",
        "    history = []\n",
        "    while not done:\n",
        "      if t > 0:\n",
        "        a = π[s]\n",
        "      s_prime, R, done = self.step(s, a)\n",
        "      history.append((s, a, R))\n",
        "      s = s_prime\n",
        "      t += 1\n",
        "\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can access the transition probability matrices and the reward vector as follows:"
      ],
      "metadata": {
        "id": "xgAiJxSnZtkH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-rfjh_37kmX"
      },
      "source": [
        "mdp = MDP()\n",
        "P = mdp.P\n",
        "R = mdp.R\n",
        "\n",
        "\n",
        "s = 2 # current state\n",
        "s_prime = 4  # next state\n",
        "a = 1  # chosen action\n",
        "\n",
        "# Probability of transition from state s (2) to s_prime (4) if action == a (1):\n",
        "print(P[a][s, s_prime])\n",
        "\n",
        "# Reward at state s if action = a\n",
        "print(R[a][s])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWfK-47V8I08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0092daa-8704-4ca7-d575-19e729a7552d"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class VaccineMDP:\n",
        "    def __init__(self):\n",
        "        # States: 0 (initial), 1 (promising phase 1), 2 (disappointing phase 1),\n",
        "        # 3 (success), 4 (failure)\n",
        "        self.states = [0, 1, 2, 3, 4]\n",
        "\n",
        "        # Actions: 0 (no additional investment), 1 (invest $100,000)\n",
        "        self.actions = [0, 1]\n",
        "\n",
        "        # Discount factor\n",
        "        self.gamma = 0.996\n",
        "\n",
        "        # Transition probabilities without additional investment (P0)\n",
        "        self.P0 = np.array([\n",
        "            [0.5, 0.15, 0.15, 0, 0.20],  # state 0\n",
        "            [0, 0.5, 0.0, 0.25, 0.25],   # state 1\n",
        "            [0, 0, 0.15, 0.05, 0.80],    # state 2\n",
        "            [0, 0, 0, 0, 1],             # state 3 (success)\n",
        "            [0, 0, 0, 0, 1]              # state 4 (failure)\n",
        "        ])\n",
        "\n",
        "        # Transition probabilities with additional investment (P1)\n",
        "        self.P1 = np.array([\n",
        "            [0.5, 0.25, 0.15, 0, 0.10],  # state 0\n",
        "            [0, 0.5, 0.0, 0.35, 0.15],   # state 1\n",
        "            [0, 0, 0.20, 0.05, 0.75],    # state 2\n",
        "            [0, 0, 0, 0, 1],             # state 3 (success)\n",
        "            [0, 0, 0, 0, 1]              # state 4 (failure)\n",
        "        ])\n",
        "\n",
        "        # Rewards without additional investment (R0)\n",
        "        self.R0 = np.array([0, 0, 0, 10, 0])\n",
        "\n",
        "        # Rewards with additional investment (R1)\n",
        "        # Includes the cost of investment (-0.1 = -$100,000)\n",
        "        self.R1 = np.array([-0.1, -0.1, -0.1, 10, 0])\n",
        "\n",
        "    def value_iteration(self, epsilon=1e-6, max_iterations=1000):\n",
        "        \"\"\"\n",
        "        Perform value iteration to compute the optimal value function\n",
        "        \"\"\"\n",
        "        # Initialize value function\n",
        "        V = np.zeros(len(self.states))\n",
        "\n",
        "        for _ in range(max_iterations):\n",
        "            # Store previous value function to check convergence\n",
        "            V_prev = V.copy()\n",
        "\n",
        "            # Update value for each state\n",
        "            for s in range(len(self.states)):\n",
        "                # Compute value for both actions (no investment and investment)\n",
        "                # If in terminal states (3 or 4), value is just the reward\n",
        "                if s in [3, 4]:\n",
        "                    V[s] = self.R0[s]\n",
        "                    continue\n",
        "\n",
        "                # Compute expected values for both actions\n",
        "                action_values = []\n",
        "                for a in self.actions:\n",
        "                    # Choose appropriate transition and reward matrices\n",
        "                    P = self.P0 if a == 0 else self.P1\n",
        "                    R = self.R0 if a == 0 else self.R1\n",
        "\n",
        "                    # Compute expected value\n",
        "                    expected_value = R[s]\n",
        "                    for s_next in range(len(self.states)):\n",
        "                        expected_value += self.gamma * P[s, s_next] * V_prev[s_next]\n",
        "\n",
        "                    action_values.append(expected_value)\n",
        "\n",
        "                # Choose the action with maximum value\n",
        "                V[s] = max(action_values)\n",
        "\n",
        "            # Check for convergence\n",
        "            if np.max(np.abs(V - V_prev)) < epsilon:\n",
        "                break\n",
        "\n",
        "        return V\n",
        "\n",
        "# Run the value iteration\n",
        "mdp = VaccineMDP()\n",
        "optimal_values = mdp.value_iteration()\n",
        "\n",
        "# Print the optimal value function\n",
        "print(\"Optimal Value Function (V):\")\n",
        "for s, value in enumerate(optimal_values):\n",
        "    print(f\"State {s}: {value:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Value Function (V):\n",
            "State 0: 3.3207\n",
            "State 1: 6.7450\n",
            "State 2: 0.5855\n",
            "State 3: 10.0000\n",
            "State 4: 0.0000\n"
          ]
        }
      ]
    }
  ]
}